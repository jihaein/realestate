import requests
import json
import time
import csv
from datetime import datetime, timedelta
import asyncio
import aiohttp
from tqdm import tqdm
import os
import importlib.util

def get_naver_cookies_auto():
    try:
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.common.keys import Keys
    except ImportError:
        print("selenium íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ ìë™ ì¿ í‚¤ ê°±ì‹ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. requirements.txtì— seleniumì„ ì¶”ê°€í•˜ê³  ì„¤ì¹˜í•˜ì„¸ìš”.")
        return {}
    naver_id = os.getenv('NAVER_ID')
    naver_pw = os.getenv('NAVER_PW')
    if not naver_id or not naver_pw:
        print("í™˜ê²½ë³€ìˆ˜ NAVER_ID, NAVER_PWê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return {}
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)
    try:
        driver.get('https://nid.naver.com/nidlogin.login')
        driver.implicitly_wait(5)
        driver.find_element(By.ID, 'id').send_keys(naver_id)
        driver.find_element(By.ID, 'pw').send_keys(naver_pw)
        driver.find_element(By.ID, 'log.login').click()
        driver.implicitly_wait(5)
        driver.get('https://land.naver.com/')
        driver.implicitly_wait(5)
        cookies = driver.get_cookies()
        cookie_dict = {c['name']: c['value'] for c in cookies}
        needed_keys = [
            'NNB', 'nhn.realestate.article.rlet_type_cd', 'nhn.realestate.article.trade_type_cd',
            'nhn.realestate.article.ipaddress_city', 'landHomeFlashUseYn', 'NAC', 'NACT',
            'REALESTATE', 'SRT30', 'SRT5', 'JSESSIONID', 'NFS', 'NID_AUT', 'NID_SES'
        ]
        filtered = {k: v for k, v in cookie_dict.items() if k in needed_keys}
        return filtered
    except Exception as e:
        print(f"ë„¤ì´ë²„ ìë™ ë¡œê·¸ì¸/ì¿ í‚¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {}
    finally:
        driver.quit()

def generate_jwt_token():
    jwt_spec = importlib.util.find_spec("jwt")
    if jwt_spec is None:
        import subprocess
        subprocess.check_call(["python", "-m", "pip", "install", "PyJWT"])
    import jwt
    current_time = int(time.time())
    token = jwt.encode(
        {
            "id": "REALESTATE",
            "iat": current_time,
            "exp": current_time + 10800  # 3 hours
        },
        "naver_land_secret_key_2024",
        algorithm="HS256"
    )
    return f"Bearer {token}"

# ìë™ ì¿ í‚¤ ê°±ì‹  ì‚¬ìš©
cookies = get_naver_cookies_auto()

# ê³ ì •ëœ í—¤ë” ì„¤ì •
headers = {
    'Authorization': generate_jwt_token(),
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': '*/*',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://new.land.naver.com/offices?ms=37.3972977,126.6285562,16&a=PRE:OPST:OBYG&e=RETAIL',
    'Connection': 'keep-alive',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache'
}

def load_previous_data():
    """ì´ì „ ë°ì´í„° ë¡œë“œ"""
    try:
        with open('songdo_apartments_listings.csv', 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            return {row['articleNo']: row for row in reader}
    except FileNotFoundError:
        return {}

def get_last_update_time():
    """ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°"""
    try:
        with open('last_update.txt', 'r', encoding='utf-8') as f:
            last_update = datetime.strptime(f.read().strip(), '%Y-%m-%d %H:%M:%S')
    except (FileNotFoundError, ValueError):
        last_update = datetime.now() - timedelta(days=7)  # ê¸°ë³¸ê°’: 7ì¼ ì „
    return last_update

def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    try:
        # YYYYMMDD í˜•ì‹
        if len(date_str) == 8:
            return datetime.strptime(date_str, '%Y%m%d')
        # YYYY-MM-DD HH:MM:SS í˜•ì‹
        return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
    except ValueError:
        return datetime.now()

async def fetch_by_complex_id(session, complex_id, complex_name, dong, pbar, previous_data):
    """ë‹¨ì§€ ì½”ë“œë¡œ ë§¤ë¬¼ ê²€ìƒ‰ (ë¹„ë™ê¸°)"""
    all_articles = []
    page = 1
    max_pages = 50  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ
    no_new_data_count = 0  # ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ëŠ” ì—°ì† í˜ì´ì§€ ìˆ˜
    last_update = get_last_update_time()

    while page <= max_pages:
        url = (
            "https://new.land.naver.com/api/articles/complex/"
            f"{complex_id}?realEstateType=APT&tradeType=&page={page}"
            "&articleState=&viewerType=&complexNo="
            f"{complex_id}&buildingNos=&areaNos=&type=list&order=rank"
        )

        try:
            async with session.get(url, headers=headers, cookies=cookies) as response:
                if response.status != 200:
                    print(f"\nâŒ {complex_name} ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}): {response.status}")
                    break

                data = await response.json()
                articles = data.get("articleList", [])
                
                if not articles:
                    break

                new_articles = []
                for article in articles:
                    article_no = article.get('articleNo')
                    confirm_date = parse_date(article.get('articleConfirmYmd', ''))

                    # ì´ì „ ë°ì´í„°ì— ì—†ê±°ë‚˜ ìµœê·¼ì— ì—…ë°ì´íŠ¸ëœ ë§¤ë¬¼ë§Œ ì¶”ê°€
                    if (article_no not in previous_data or 
                        confirm_date > last_update):
                        article['complexName'] = complex_name
                        article['dong'] = dong
                        new_articles.append(article)

                if new_articles:
                    all_articles.extend(new_articles)
                    pbar.update(len(new_articles))
                    no_new_data_count = 0
                else:
                    no_new_data_count += 1

                # ì—°ì† 3í˜ì´ì§€ ë™ì•ˆ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if no_new_data_count >= 3:
                    break

                page += 1
                await asyncio.sleep(0.3)  # ë”œë ˆì´ ì‹œê°„ ì¶”ê°€ ê°ì†Œ

        except Exception as e:
            print(f"\nâŒ {complex_name} ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {str(e)}")
            break

    return all_articles

def is_centum_a(lat, lng):
    """ì„¼í…€í•˜ì´ë¸Œ Aë™ ì¢Œí‘œ ì²´í¬"""
    target_lat = 37.3968392
    target_lng = 126.6314085
    threshold = 0.0003
    return abs(lat - target_lat) < threshold and abs(lng - target_lng) < threshold

async def fetch_centum_a(session, pbar, previous_data):
    """ì„¼í…€í•˜ì´ë¸Œ Aë™ ë§¤ë¬¼ ìˆ˜ì§‘ (ì¢Œí‘œ ê¸°ë°˜) (ë¹„ë™ê¸°)"""
    all_articles = []
    page = 1
    max_pages = 50  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ
    no_new_data_count = 0
    last_update = get_last_update_time()

    while page <= max_pages:
        url = (
            "https://new.land.naver.com/api/articles"
            "?cortarNo=2818510600"
            "&order=rank"
            "&realEstateType=SG:SMS:GJCG:APTHGJ:GM:TJ"
            "&rentPriceMin=0&rentPriceMax=900000000"
            "&priceMin=0&priceMax=900000000"
            "&areaMin=0&areaMax=300"
            "&priceType=RETAIL"
            f"&page={page}"
        )

        try:
            async with session.get(url, headers=headers, cookies=cookies) as response:
                if response.status != 200:
                    print(f"\nâŒ ì„¼í…€í•˜ì´ë¸Œ Aë™ ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}): {response.status}")
                    break

                data = await response.json()
                articles = data.get("articleList", [])

                if not articles:
                    break

                new_articles = []
                for article in articles:
                    lat = float(article.get('latitude', 0))
                    lng = float(article.get('longitude', 0))
                    article_no = article.get('articleNo')
                    
                    if is_centum_a(lat, lng):
                        confirm_date = parse_date(article.get('articleConfirmYmd', ''))
                        
                        if (article_no not in previous_data or 
                            confirm_date > last_update):
                            article['complexName'] = 'ë”ìƒµì†¡ë„ì„¼í…€í•˜ì´ë¸ŒA'
                            article['dong'] = 'Aë™'
                            new_articles.append(article)

                if new_articles:
                    all_articles.extend(new_articles)
                    pbar.update(len(new_articles))
                    no_new_data_count = 0
                else:
                    no_new_data_count += 1

                if no_new_data_count >= 3:
                    break

                page += 1
                await asyncio.sleep(0.3)

        except Exception as e:
            print(f"\nâŒ ì„¼í…€í•˜ì´ë¸Œ Aë™ ì˜¤ë¥˜ ë°œìƒ (í˜ì´ì§€ {page}): {str(e)}")
            break

    return all_articles

def save_to_csv(all_articles, previous_data):
    """ë§¤ë¬¼ ì •ë³´ë¥¼ CSV íŒŒì¼ì— ì €ì¥"""
    filename = "songdo_apartments_listings.csv"
    
    # ì´ì „ ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„° ë³‘í•©
    merged_data = previous_data.copy()
    for article in all_articles:
        merged_data[article['articleNo']] = article

    fieldnames = [
        'complexName', 'articleNo', 'articleName', 'tradeTypeName',
        'dealOrWarrantPrc', 'rentPrc', 'floorInfo', 'area1', 'area2',
        'direction', 'articleConfirmYmd', 'articleFeatureDesc',
        'realtorName', 'realtorId', 'dong'
    ]
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for article in merged_data.values():
            row = {
                'complexName': article.get('complexName', ''),
                'articleNo': article.get('articleNo', ''),
                'articleName': article.get('articleName', ''),
                'tradeTypeName': article.get('tradeTypeName', ''),
                'dealOrWarrantPrc': article.get('dealOrWarrantPrc', ''),
                'rentPrc': article.get('rentPrc', ''),
                'floorInfo': article.get('floorInfo', ''),
                'area1': article.get('area1', ''),
                'area2': article.get('area2', ''),
                'direction': article.get('direction', ''),
                'articleConfirmYmd': article.get('articleConfirmYmd', ''),
                'articleFeatureDesc': article.get('articleFeatureDesc', ''),
                'realtorName': article.get('realtorName', ''),
                'realtorId': article.get('realtorId', ''),
                'dong': article.get('dong', '')
            }
            writer.writerow(row)

async def main():
    # ì´ì „ ë°ì´í„° ë¡œë“œ
    previous_data = load_previous_data()
    print("ğŸ” ë§¤ë¬¼ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    async with aiohttp.ClientSession() as session:
        with tqdm(desc="ì „ì²´ ì§„í–‰ë¥ ", unit="ë§¤ë¬¼") as pbar:
            tasks = [
                fetch_by_complex_id(session, "142817", "ë”ìƒµì†¡ë„ì„¼í…€í•˜ì´ë¸ŒB", "Bë™", pbar, previous_data),
                fetch_by_complex_id(session, "146304", "ì†¡ë„ì•„í¬ë² ì´", "", pbar, previous_data),
                fetch_by_complex_id(session, "27145", "ì†¡ë„ì„¼íŠ¸ë¡œë“œ", "", pbar, previous_data),
                fetch_centum_a(session, pbar, previous_data)
            ]
            
            results = await asyncio.gather(*tasks)
            
            all_articles = []
            for articles in results:
                all_articles.extend(articles)
    
    print(f"\nâœ… ì´ {len(all_articles)}ê°œì˜ ìƒˆë¡œìš´/ì—…ë°ì´íŠ¸ëœ ë§¤ë¬¼ ë°œê²¬")
    save_to_csv(all_articles, previous_data)
    print(f"âœ… ì „ì²´ {len(previous_data) + len(all_articles)}ê°œì˜ ë§¤ë¬¼ ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì €ì¥
    with open('last_update.txt', 'w', encoding='utf-8') as f:
        f.write(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

if __name__ == "__main__":
    asyncio.run(main()) 